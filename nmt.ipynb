{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python自带\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 科学计算\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn a s nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP 相关\n",
    "import jieba\n",
    "import torchtext\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize_zh(input):\n",
    "    return list(jieba.cut(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = torchtext.data.Field(tokenize=word_tokenize, eos_token='<eos>')\n",
    "TRG = torchtext.data.Field(tokenize=word_tokenize_zh, init_token='<sos>', eos_token='<eos>')\n",
    "\n",
    "train_data = torchtext.datasets.TranslationDataset(\n",
    "    path='data/news-commentary-v12.zh-en',\n",
    "    exts=('.en', '.zh'),\n",
    "    fields=(SRC, TRG)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227383\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = torchtext.datasets.TranslationDataset(\n",
    "    path='data/newsdev2017-enzh',\n",
    "    exts=('.en', '.zh'),\n",
    "    fields=(SRC, TRG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torchtext.datasets.TranslationDataset(\n",
    "    path='data/newstest2017-enzh',\n",
    "    exts=('.en', '.zh'),\n",
    "    fields=(SRC, TRG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data.src, dev_data.src, test_data.src, min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG.build_vocab(train_data.trg, dev_data.trg, test_data.trg, min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95171\n",
      "91404\n"
     ]
    }
   ],
   "source": [
    "print(len(SRC.vocab.__dict__['freqs']))\n",
    "print(len(TRG.vocab.__dict__['freqs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.vocab.load_vectors(torchtext.vocab.Vectors('data/glove.840B.300d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG.vocab.load_vectors(torchtext.vocab.Vectors('data/sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''Encoder(bi-GRU)\n",
    "    '''\n",
    "    def __init__(self, pretrained_embed, padding_idx, fix, hidden_size,\n",
    "                 n_layers=1, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embed)\n",
    "        self.embedding.padding_idx = padding_idx\n",
    "        if fix:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.gru = nn.GRU(self.embedding.embedding_dim, hidden_size, n_layers,\n",
    "                            dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, src, hidden=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "            src: input word index\n",
    "            hidden: h_t-1 (num_layers * num_directions, batch, hidden_size)\n",
    "        Outputs:\n",
    "            output: [T*B*H]\n",
    "            hidden: h_t\n",
    "        '''\n",
    "        embeded = self.embedding(src)\n",
    "        outputs, hidden = self.gru(embeded, hidden)\n",
    "        \n",
    "        # Sum bi-lstm outputs\n",
    "        output = (outputs[:, :, :self.hidden_size] + \n",
    "                   outputs[:, :, self.hidden_size:])\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class ConcatAttn(nn.Module):\n",
    "    '''Attention(concat)\n",
    "    Params:\n",
    "        hidden_size: hidden size\n",
    "    '''\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ConcatAttn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1.0 / sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, hidden, encoder_output):\n",
    "        '''\n",
    "        Inputs:\n",
    "            hidden: [1*B*H] \n",
    "            encoder_output: [T*B*H]\n",
    "        Outputs:\n",
    "            energy: normalised weights [B*1*T]\n",
    "        '''\n",
    "        # Expand hidden [1*B*H] -> [T*B*H] -> [B*T*H]\n",
    "        hidden = hidden.repeat(encoder_output.size(0), 1, 1).transpose(0, 1)\n",
    "\n",
    "        # Transfer encoder_output to [B*T*H]\n",
    "        encoder_output = encoder_output.transpose(0, 1)\n",
    "\n",
    "        # Calculate energy and normalise  [B*1*T]\n",
    "        attn_energy = self.score(hidden, encoder_output)\n",
    "        return F.softmax(attn_energy, dim=2)\n",
    "\n",
    "    def score(self, hidden, encoder_output):\n",
    "        '''\n",
    "        Inputs:\n",
    "            hidden: [B*T*H]\n",
    "            encoder_output: [B*T*H]\n",
    "        Outputs:\n",
    "            attn_energy: weights [B*T]\n",
    "        '''\n",
    "        # Project vectors [B*T*2H] -> [B*T*H] -> [B*H*T]\n",
    "        energy = self.attn(torch.cat([hidden, encoder_output], 2))\n",
    "        energy = energy.transpose(1, 2)\n",
    "        \n",
    "        # Expend v  [H] -> [B*H] -> [B*1*H]\n",
    "        v = self.v.repeat(encoder_output.size(0), 1).unsqueeze(1)\n",
    "        \n",
    "        # [B*1*H] * [B*H*T] -> [B*1*T]\n",
    "        attn_energy = torch.bmm(v, energy)\n",
    "        return attn_energy\n",
    "\n",
    "        \n",
    "class BilinearAttn(nn.Module):\n",
    "    '''Attention(bilinear)\n",
    "    Params:\n",
    "        hidden_size: hidden size\n",
    "    '''\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BilinearAttn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bilinear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    \n",
    "    def forward(self, hidden, encoder_output):\n",
    "        '''\n",
    "        Inputs:\n",
    "            hidden: [1*B*H] \n",
    "            encoder_output: [T*B*H]\n",
    "        Outputs:\n",
    "            energy: normalised weights [B*1*T]\n",
    "        '''\n",
    "        # [T*B*H] -> [T*B*H] -> [B*H*T]\n",
    "        wh = self.bilinear(encoder_output).permute(1, 2, 0)\n",
    "        \n",
    "        # [1*B*H] -> [B*1*H] x [B*H*T] => [B*1*T]\n",
    "        score = hidden.transpose(0, 1).bmm(wh)\n",
    "        \n",
    "        return F.softmax(score, dim=2)\n",
    "    \n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''Decoder(bi-GRU)\n",
    "    '''\n",
    "    def __init__(self, pretrained_embed, padding_idx, hidden_size, fix, output_size,\n",
    "                 n_layers=1, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embed)\n",
    "        self.embedding.padding_idx = padding_idx\n",
    "        if fix:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "            \n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        \n",
    "        self.attention = BilinearAttn(hidden_size)\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            self.embedding.embedding_dim,\n",
    "            hidden_size,\n",
    "            n_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_size * 2, hidden_size * 2, bias=False)\n",
    "        self.linear2 = nn.Linear(hidden_size * 2, output_size, bias=False)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_output):\n",
    "        '''\n",
    "        Inputs:\n",
    "            input: [B]\n",
    "            hidden: [layers*B*H]\n",
    "            encoder_output: [T*B*H]\n",
    "        Outputs:\n",
    "            p: [B*O]\n",
    "            hidden: [layers*B*H]\n",
    "        '''\n",
    "        # [B] -> [B*E] -> [1*B*E]\n",
    "        embeded = self.embedding(input).unsqueeze(0) \n",
    "            \n",
    "        # [1*B*H], [layers*B*H]\n",
    "        output, hidden = self.gru(embeded, hidden)\n",
    "        \n",
    "        # ht: [B*H]  the last layer\n",
    "        ht = hidden[-1, :, :]\n",
    "        \n",
    "        # [1*B*T] and [T*B*H] -> [B*1*T]\n",
    "        attn_weights = self.attention(ht.unsqueeze(0), encoder_output)\n",
    "        \n",
    "        # [B*1*T] x [B*T*H] => [B*1*H] -> [B*H]\n",
    "        c = attn_weights.bmm(encoder_output.transpose(0, 1)).squeeze(1)\n",
    "        \n",
    "        # concat c and h => [B*2H] => [B*H] \n",
    "        attn_vector = F.tanh(self.linear1(\n",
    "            torch.cat([c, ht], dim=1)\n",
    "        ))\n",
    "        \n",
    "        # [B*H] -> [B*O]\n",
    "        p = F.softmax(self.linear2(attn_vector), dim=1)\n",
    "\n",
    "        return p, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = torchtext.data.BucketIterator(\n",
    "    dataset=train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dev_iter = torchtext.data.BucketIterator(\n",
    "    dataset=dev_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_iter = torchtext.data.BucketIterator(\n",
    "    dataset=test_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_TOKEN = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epoch, encoder, decoder, encoder_optimizer, decoder_optimizer ,criterion, eval_steps, train_iter, dev_iter, device):\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    \n",
    "    step = 0\n",
    "    train_loss = 0.0\n",
    "    lowest_loss = 1e5\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    for e in range(epoch):\n",
    "        train_iter.init_epoch()\n",
    "        for train_batch in iter(train_iter):\n",
    "            step += 1\n",
    "            \n",
    "            # [T*B]\n",
    "            src = train_batch.src.to(device)\n",
    "            trg = train_batch.trg.to(device)\n",
    "            \n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            \n",
    "            # encoder\n",
    "            encoder_output, hidden = encoder(src)\n",
    "            \n",
    "            # decoder            \n",
    "            hidden = hidden[:decoder.n_layers]\n",
    "            decoder_input = trg[0] # SOS\n",
    "            \n",
    "            \n",
    "            loss = 0.0\n",
    "            for i in range(trg.size(0) - 1):\n",
    "                p, hidden = decoder(\n",
    "                    decoder_input, hidden, encoder_output\n",
    "                )\n",
    "                loss += criterion(p, trg[i+1])\n",
    "                decoder_input = trg[i+1]\n",
    "\n",
    "                \n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            \n",
    "            if (step % eval_steps) == 0:\n",
    "                with torch.no_grad():\n",
    "                    encoder.eval()\n",
    "                    decoder.eval()\n",
    "                    \n",
    "                    dev_iter.init_epoch()\n",
    "                    dev_loss = 0.0\n",
    "                    dev_step = 0\n",
    "                    for dev_batch in iter(dev_iter):\n",
    "                        dev_step += 1\n",
    "                        dev_src = dev_batch.src.to(device)\n",
    "                        dev_trg = dev_batch.trg.to(device)\n",
    "                        \n",
    "                        encoder_output, hidden = encoder(src)\n",
    "                        hidden = hidden[:decoder.n_layers]\n",
    "                        decoder_input = trg[0]\n",
    "                        for i in range(trg.size(0) - 1):\n",
    "                            p, hidden = decoder(\n",
    "                                decoder_input, hidden, encoder_output\n",
    "                            )\n",
    "                            dev_loss += criterion(p, trg[i+1])\n",
    "                            decoder_input = trg[i+1] \n",
    "                    train_loss /= eval_steps\n",
    "                    dev_loss /= dev_step\n",
    "                    print(\"epoch {0} steps {1} train_loss {2} dev_loss{3}\".format(\n",
    "                        e, step, train_loss, dev_loss\n",
    "                    ))\n",
    "                    \n",
    "                    if dev_loss < lowest_loss:\n",
    "                        dev_loss = lowest_loss\n",
    "                        save(\n",
    "                            encoder=encoder,\n",
    "                            decoder=decoder,\n",
    "                            info={'steps':step, 'epoch':e, 'train_loss':train_loss, 'dev_loss':dev_loss}\n",
    "                        )\n",
    "                    \n",
    "                    train_loss = 0.0\n",
    "                    encoder.train()\n",
    "                    decoder.train()\n",
    "\n",
    "def save(encoder, decoder, info):\n",
    "    torch.save(info, 'best_model.info')\n",
    "    torch.save(encoder, 'best_encoder.m')\n",
    "    torch.save(decoder, 'best_decoder.m')\n",
    "    \n",
    "def load():\n",
    "    encoder = torch.load('best_encoder.m')\n",
    "    decoder = torch.load('best_decoder.m')\n",
    "    info = torch.load('best_model.info')\n",
    "    return encoder, decoder, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n"
     ]
    }
   ],
   "source": [
    "choise = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(choise + \" is available\")\n",
    "device = torch.device(choise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-155-53a018989bc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m training(epoch=20, encoder=encoder, decoder=decoder, encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer, \n\u001b[0;32m---> 11\u001b[0;31m          criterion=criterion, eval_steps=500, train_iter=train_iter, dev_iter=dev_iter, device=device)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-153-764fd2322e84>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(epoch, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, eval_steps, train_iter, dev_iter, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_optimizer\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(pretrained_embed=SRC.vocab.vectors, padding_idx=SRC.vocab.stoi[SRC.pad_token], fix=True, hidden_size=128, dropout=0.3, n_layers=4)\n",
    "decoder = Decoder(pretrained_embed=TRG.vocab.vectors, padding_idx=TRG.vocab.stoi[TRG.pad_token], fix=True, hidden_size=128, dropout=0.3, n_layers=4, output_size=len(TRG.vocab.freqs))\n",
    "\n",
    "encoder_optimizer = optim.Adam(filter(lambda p: p.requires_grad, encoder.parameters()), lr=1e-3)\n",
    "decoder_optimizer = optim.Adam(filter(lambda p: p.requires_grad, decoder.parameters()), lr=1e-3)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "\n",
    "training(epoch=20, encoder=encoder, decoder=decoder, encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer, \n",
    "         criterion=criterion, eval_steps=500, train_iter=train_iter, dev_iter=dev_iter, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
