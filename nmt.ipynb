{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python自带\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from lxml import etree\n",
    "from queue import Queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 科学计算\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from tensorboardX import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP 相关\n",
    "import jieba\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画图\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(src):\n",
    "    return src.split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = torchtext.data.Field(tokenize=tokenizer, eos_token='<eos>')\n",
    "TRG = torchtext.data.Field(tokenize=tokenizer, init_token='<sos>', eos_token='<eos>')\n",
    "\n",
    "dataset = torchtext.datasets.TranslationDataset(\n",
    "    path='data/NEU',\n",
    "    exts=('.en.tok.bpe', '.zh.tok'),\n",
    "    fields=(SRC, TRG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, dev_data = dataset.split(split_ratio=0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data, test_data = dev_data.split(split_ratio=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1959416\n",
      "19994\n",
      "19994\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "print(len(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data.src, dev_data.src, test_data.src, max_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG.build_vocab(train_data.trg, dev_data.trg, test_data.trg, max_size=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32115\n",
      "50004\n"
     ]
    }
   ],
   "source": [
    "print(len(SRC.vocab.itos))\n",
    "print(len(TRG.vocab.itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['souri',\n",
       " 'staur@@',\n",
       " 'susp@@',\n",
       " 'tellig@@',\n",
       " 'therto',\n",
       " 'thwest',\n",
       " 'tremend@@',\n",
       " 'ublic',\n",
       " 'usetts',\n",
       " 'warri@@']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SRC.vocab.itos[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.vocab.load_vectors(torchtext.vocab.Vectors('data/glove.840B.300d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG.vocab.load_vectors(torchtext.vocab.Vectors('data/sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''Encoder(bi-GRU)\n",
    "    '''\n",
    "    def __init__(self, pretrained_embed, padding_idx, fix, hidden_size,\n",
    "                 n_layers=1, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embed)\n",
    "        self.embedding.padding_idx = padding_idx\n",
    "        if fix:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.gru = nn.GRU(self.embedding.embedding_dim, hidden_size, n_layers,\n",
    "                            dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, src, hidden=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "            src: input word index\n",
    "            hidden: h_t-1 (num_layers * num_directions, batch, hidden_size)\n",
    "        Outputs:\n",
    "            output: [T*B*H]\n",
    "            hidden: h_t\n",
    "        '''\n",
    "        embeded = self.embedding(src)\n",
    "        outputs, hidden = self.gru(embeded, hidden)\n",
    "        \n",
    "        # Sum bi-lstm outputs\n",
    "        output = (outputs[:, :, :self.hidden_size] + \n",
    "                   outputs[:, :, self.hidden_size:])\n",
    "    \n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class ConcatAttn(nn.Module):\n",
    "    '''Attention(concat)\n",
    "    Params:\n",
    "        hidden_size: hidden size\n",
    "    '''\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ConcatAttn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1.0 / sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, hidden, encoder_output):\n",
    "        '''\n",
    "        Inputs:\n",
    "            hidden: [1*B*H] \n",
    "            encoder_output: [T*B*H]\n",
    "        Outputs:\n",
    "            energy: normalised weights [B*1*T]\n",
    "        '''\n",
    "        # Expand hidden [1*B*H] -> [T*B*H] -> [B*T*H]\n",
    "        hidden = hidden.repeat(encoder_output.size(0), 1, 1).transpose(0, 1)\n",
    "\n",
    "        # Transfer encoder_output to [B*T*H]\n",
    "        encoder_output = encoder_output.transpose(0, 1)\n",
    "\n",
    "        # Calculate energy and normalise  [B*1*T]\n",
    "        attn_energy = self.score(hidden, encoder_output)\n",
    "        return F.softmax(attn_energy, dim=2)\n",
    "\n",
    "    def score(self, hidden, encoder_output):\n",
    "        '''\n",
    "        Inputs:\n",
    "            hidden: [B*T*H]\n",
    "            encoder_output: [B*T*H]\n",
    "        Outputs:\n",
    "            attn_energy: weights [B*T]\n",
    "        '''\n",
    "        # Project vectors [B*T*2H] -> [B*T*H] -> [B*H*T]\n",
    "        energy = self.attn(torch.cat([hidden, encoder_output], 2))\n",
    "        energy = energy.transpose(1, 2)\n",
    "        \n",
    "        # Expend v  [H] -> [B*H] -> [B*1*H]\n",
    "        v = self.v.repeat(encoder_output.size(0), 1).unsqueeze(1)\n",
    "        \n",
    "        # [B*1*H] * [B*H*T] -> [B*1*T]\n",
    "        attn_energy = torch.bmm(v, energy)\n",
    "        return attn_energy\n",
    "\n",
    "        \n",
    "class BilinearAttn(nn.Module):\n",
    "    '''Attention(bilinear)\n",
    "    Params:\n",
    "        hidden_size: hidden size\n",
    "    '''\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BilinearAttn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bilinear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    \n",
    "    def forward(self, hidden, encoder_output):\n",
    "        '''\n",
    "        Inputs:\n",
    "            hidden: [1*B*H] \n",
    "            encoder_output: [T*B*H]\n",
    "        Outputs:\n",
    "            energy: normalised weights [B*1*T]\n",
    "        '''\n",
    "        # [T*B*H] -> [T*B*H] -> [B*H*T]\n",
    "        wh = self.bilinear(encoder_output).permute(1, 2, 0)\n",
    "        \n",
    "        # [1*B*H] -> [B*1*H] x [B*H*T] => [B*1*T]\n",
    "        score = hidden.transpose(0, 1).bmm(wh)\n",
    "        \n",
    "        return F.softmax(score, dim=2)\n",
    "    \n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''Decoder(bi-GRU)\n",
    "    '''\n",
    "    def __init__(self, pretrained_embed, padding_idx, hidden_size, fix, output_size,\n",
    "                 n_layers=1, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embed)\n",
    "        self.embedding.padding_idx = padding_idx\n",
    "        if fix:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "            \n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        \n",
    "        self.attention = BilinearAttn(hidden_size)\n",
    "        \n",
    "        self.gru = nn.GRU(\n",
    "            self.embedding.embedding_dim,\n",
    "            hidden_size,\n",
    "            n_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        self.linear1 = nn.Linear(hidden_size * 2, hidden_size * 2, bias=False)\n",
    "        self.linear2 = nn.Linear(hidden_size * 2, output_size, bias=False)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_output):\n",
    "        '''\n",
    "        Inputs:\n",
    "            input: [B]\n",
    "            hidden: [layers*B*H]\n",
    "            encoder_output: [T*B*H]\n",
    "        Outputs:\n",
    "            p: [B*O]\n",
    "            hidden: [layers*B*H]\n",
    "        '''\n",
    "        # [B] -> [B*E] -> [1*B*E]\n",
    "        embeded = self.embedding(input).unsqueeze(0) \n",
    "            \n",
    "        # [1*B*H], [layers*B*H]\n",
    "        output, hidden = self.gru(embeded, hidden)\n",
    "        \n",
    "        # ht: [B*H]  the last layer\n",
    "        ht = hidden[-1, :, :]\n",
    "        \n",
    "        # [1*B*T] and [T*B*H] -> [B*1*T]\n",
    "        attn_weights = self.attention(ht.unsqueeze(0), encoder_output)\n",
    "        \n",
    "        # [B*1*T] x [B*T*H] => [B*1*H] -> [B*H]\n",
    "        c = attn_weights.bmm(encoder_output.transpose(0, 1)).squeeze(1)\n",
    "        \n",
    "        # concat c and h => [B*2H] => [B*H] \n",
    "        attn_vector = torch.tanh(self.linear1(\n",
    "            torch.cat([c, ht], dim=1)\n",
    "        ))\n",
    "        \n",
    "        # [B*H] -> [B*O]\n",
    "        p = F.log_softmax(self.linear2(attn_vector), dim=1)\n",
    "        \n",
    "        return p, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = torchtext.data.BucketIterator(\n",
    "    dataset=train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dev_iter = torchtext.data.BucketIterator(\n",
    "    dataset=dev_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_iter = torchtext.data.BucketIterator(\n",
    "    dataset=test_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(epoch, encoder, decoder, encoder_optimizer, decoder_optimizer ,criterion, eval_steps, train_iter, dev_iter, device, writer, warmup):\n",
    "    encoder.to(device)\n",
    "    decoder.to(device)\n",
    "    \n",
    "    step = 0\n",
    "    train_loss = 0.0\n",
    "    lowest_loss = 1e5\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    for e in range(epoch):\n",
    "        if e >= warmup:\n",
    "            encoder.embedding.weight.requires_grad = True\n",
    "            decoder.embedding.weight.requires_grad = True\n",
    "            encoder_optimizer = optim.Adam(filter(lambda p: p.requires_grad, encoder.parameters()), lr=1e-4)\n",
    "            decoder_optimizer = optim.Adam(filter(lambda p: p.requires_grad, decoder.parameters()), lr=1e-4)\n",
    "        \n",
    "        train_iter.init_epoch()\n",
    "        for train_batch in iter(train_iter):\n",
    "            step += 1\n",
    "            \n",
    "            # [T*B]\n",
    "            src = train_batch.src.to(device)\n",
    "            trg = train_batch.trg.to(device)\n",
    "            \n",
    "            encoder_optimizer.zero_grad()\n",
    "            decoder_optimizer.zero_grad()\n",
    "            \n",
    "            # encoder\n",
    "            encoder_output, hidden = encoder(src)\n",
    "            \n",
    "            # decoder            \n",
    "            hidden = hidden[:decoder.n_layers]\n",
    "            decoder_input = trg[0] # SOS\n",
    "            \n",
    "            loss = 0.0\n",
    "            for i in range(trg.size(0) - 1):\n",
    "                p, hidden, _ = decoder(\n",
    "                    decoder_input, hidden, encoder_output\n",
    "                )\n",
    "                loss += criterion(p, trg[i+1])\n",
    "                decoder_input = trg[i+1]\n",
    "                \n",
    "            loss.backward()\n",
    "            train_loss += loss.item() / (trg.size(0) - 1)\n",
    "            clip_grad_norm_(encoder.parameters(), 0.7)\n",
    "            clip_grad_norm_(decoder.parameters(), 0.7)\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            \n",
    "            if (step % eval_steps) == 0:\n",
    "                with torch.no_grad():\n",
    "                    encoder.eval()\n",
    "                    decoder.eval()\n",
    "                    \n",
    "                    dev_iter.init_epoch()\n",
    "                    dev_loss = 0.0\n",
    "                    dev_step = 0\n",
    "                    for dev_batch in iter(dev_iter):\n",
    "                        dev_step += 1\n",
    "                        dev_src = dev_batch.src.to(device)\n",
    "                        dev_trg = dev_batch.trg.to(device)\n",
    "                        \n",
    "                        encoder_output, hidden = encoder(dev_src)\n",
    "                        hidden = hidden[:decoder.n_layers]\n",
    "                        decoder_input = dev_trg[0]\n",
    "                        loss = 0.0\n",
    "                        for i in range(dev_trg.size(0) - 1):\n",
    "                            p, hidden, _ = decoder(\n",
    "                                decoder_input, hidden, encoder_output\n",
    "                            )\n",
    "                            loss += criterion(p, dev_trg[i+1])\n",
    "                            decoder_input = dev_trg[i+1] \n",
    "                        \n",
    "                        dev_loss += loss.item() / (dev_trg.size(0) - 1)\n",
    "                    \n",
    "                    train_loss /= eval_steps\n",
    "                    dev_loss /= dev_step\n",
    "                    print(\"epoch %d steps %d train_loss %.4f train_ppl %5.2f dev_loss %.4f dev_ppl %5.2f\" % (\n",
    "                        e, step, train_loss, np.exp(train_loss), dev_loss, np.exp(dev_loss)\n",
    "                    ))\n",
    "                    \n",
    "                    writer.add_scalar('train/loss', train_loss, step)\n",
    "                    writer.add_scalar('train/perplexity', np.exp(train_loss), step)\n",
    "                    writer.add_scalar('dev/loss', dev_loss, step)\n",
    "                    writer.add_scalar('dev/perplexity', np.exp(dev_loss), step)\n",
    "                    \n",
    "                    if dev_loss < lowest_loss:\n",
    "                        lowest_loss = dev_loss\n",
    "                        save(\n",
    "                            encoder=encoder,\n",
    "                            decoder=decoder,\n",
    "                            info={'steps':step, 'epoch':e, 'train_loss':train_loss, 'train_ppl':np.exp(train_loss), 'dev_loss':dev_loss, 'dev_ppl':np.exp(dev_loss)}\n",
    "                        )\n",
    "                    \n",
    "                    train_loss = 0.0\n",
    "                    encoder.train()\n",
    "                    decoder.train()\n",
    "\n",
    "def save(encoder, decoder, info):\n",
    "    torch.save(info, 'best_model.info')\n",
    "    torch.save(encoder.state_dict(), 'best_encoder.m')\n",
    "    torch.save(decoder.state_dict(), 'best_decoder.m')\n",
    "    \n",
    "def load():\n",
    "    encoder = Encoder(pretrained_embed=SRC.vocab.vectors, padding_idx=SRC.vocab.stoi[SRC.pad_token], fix=True, hidden_size=1000, dropout=0.3, n_layers=4)\n",
    "    decoder = Decoder(pretrained_embed=TRG.vocab.vectors, padding_idx=TRG.vocab.stoi[TRG.pad_token], fix=True, hidden_size=1000, dropout=0.3, n_layers=4, output_size=len(TRG.vocab.itos))\n",
    "    encoder.load_state_dict(torch.load('best_encoder.m'))\n",
    "    decoder.load_state_dict(torch.load('best_decoder.m'))\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    info = torch.load('best_model.info')\n",
    "    return encoder, decoder, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n"
     ]
    }
   ],
   "source": [
    "choise = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(choise + \" is available\")\n",
    "device = torch.device(choise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 steps 1000 train_loss 3.5138 train_ppl 33.58 dev_loss 3.1714 dev_ppl 23.84\n",
      "epoch 0 steps 2000 train_loss 3.0046 train_ppl 20.18 dev_loss 2.9001 dev_ppl 18.18\n",
      "epoch 0 steps 3000 train_loss 2.8272 train_ppl 16.90 dev_loss 2.7584 dev_ppl 15.77\n",
      "epoch 0 steps 4000 train_loss 2.7141 train_ppl 15.09 dev_loss 2.6442 dev_ppl 14.07\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(pretrained_embed=SRC.vocab.vectors, padding_idx=SRC.vocab.stoi[SRC.pad_token], fix=True, hidden_size=1000, dropout=0.3, n_layers=4)\n",
    "decoder = Decoder(pretrained_embed=TRG.vocab.vectors, padding_idx=TRG.vocab.stoi[TRG.pad_token], fix=True, hidden_size=1000, dropout=0.3, n_layers=4, output_size=len(TRG.vocab.itos))\n",
    "\n",
    "encoder_optimizer = optim.Adam(filter(lambda p: p.requires_grad, encoder.parameters()), lr=4*1e-4)\n",
    "decoder_optimizer = optim.Adam(filter(lambda p: p.requires_grad, decoder.parameters()), lr=4*1e-4)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "training(epoch=20, encoder=encoder, decoder=decoder, encoder_optimizer=encoder_optimizer, decoder_optimizer=decoder_optimizer, \n",
    "         criterion=criterion, eval_steps=1000, train_iter=train_iter, dev_iter=dev_iter, device=device, writer=writer, warmup=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'steps': 121000,\n",
       " 'epoch': 1,\n",
       " 'train_loss': 1.631992789750133,\n",
       " 'train_ppl': 5.114055808778657,\n",
       " 'dev_loss': 1.658722503808847,\n",
       " 'dev_ppl': 5.252596384636608}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder, decoder, info = load()\n",
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, hidden, previous_node, decoder_input, attn, log_prob, length):\n",
    "        self.hidden = hidden\n",
    "        self.previous_node = previous_node\n",
    "        self.decoder_input = decoder_input\n",
    "        self.attn = attn\n",
    "        self.log_prob = log_prob\n",
    "        self.length = length        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search(src, beam_width=3):\n",
    "    words = word_tokenize(src) \n",
    "    indices = [SRC.vocab.stoi[word] for word in words]\n",
    "    input_tensor = torch.LongTensor(indices).unsqueeze(1)\n",
    "    # [1*T]\n",
    "    encoder_output, hidden = encoder(input_tensor)\n",
    "    hidden = hidden[:decoder.n_layers]\n",
    "    decoder_input = torch.tensor(2).unsqueeze(0)\n",
    "    \n",
    "    node = Node(hidden, None, decoder_input, None, 0, 1)\n",
    "    q = Queue()\n",
    "    q.put(node)\n",
    "    \n",
    "    end_nodes = []\n",
    "    while not q.empty():\n",
    "        candidates = []\n",
    "        # level traversal\n",
    "        for _ in range(q.qsize()):\n",
    "            node = q.get()\n",
    "            decoder_input = node.decoder_input\n",
    "            hidden = node.hidden\n",
    "            \n",
    "            if decoder_input.item() == 3 or node.length >= 50:\n",
    "                end_nodes.append(node)\n",
    "                continue\n",
    "            \n",
    "            decoder.named_parameters\n",
    "            log_prob, hidden, attn = decoder(\n",
    "                decoder_input, hidden, encoder_output\n",
    "            )\n",
    "            \n",
    "            log_prob, indices = log_prob[0].topk(beam_width)\n",
    "            \n",
    "            for k in range(beam_width):\n",
    "                index = indices[k].unsqueeze(0)\n",
    "                log_p = log_prob[k].item()\n",
    "                child = Node(hidden, node, index, attn, node.log_prob + log_p, node.length + 1)\n",
    "                candidates.append((node.log_prob + log_p, child))\n",
    "        \n",
    "        candidates = sorted(candidates, reverse=True)\n",
    "        length = min(len(candidates), beam_width)\n",
    "        for i in range(length):\n",
    "            q.put(candidates[i][1])  \n",
    "    \n",
    "    '''\n",
    "    for node in end_nodes:\n",
    "        res = []\n",
    "        value = node.log_prob\n",
    "        while node.previous_node != None:\n",
    "            res.append(TRG.vocab.itos[node.decoder_input.item()])\n",
    "            node = node.previous_node\n",
    "        print(''.join(res[::-1]), value)'''\n",
    "    node = end_nodes[0]\n",
    "    res = []\n",
    "    attns = []\n",
    "    while node.previous_node != None:\n",
    "        res.append(TRG.vocab.itos[node.decoder_input.item()])\n",
    "        attns.append(node.attn.squeeze(0).squeeze(0).numpy().tolist())\n",
    "        node = node.previous_node\n",
    "    print(''.join(res[::-1][:-1]))\n",
    "    return attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Det', '@', '@', 'ail', 'analysis', 'suggests', 'that', 'the', 'in', '@', '@', 'breeding', 'depression', 'might', 'occur', 'during', 'the', 'period', 'of', 'seeding', 'and', 'adult', 'stage', 'or', 'of', 'flowering', 'and', 'seed', 'filled', 'stage', '.']\n",
      "Det @ @ ail analysis suggests that the in @ @ breeding depression might occur during the period of seeding and adult stage or of flowering and seed filled stage .\n",
      "分析表明近交衰退发生在幼苗至成熟之间，也可能在开花与结实期。\n",
      "<unk><unk><unk><unk><unk>成虫成虫萌发和萌发阶段的<unk><unk>。\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    index = 3\n",
    "    print(test_data[index].src)\n",
    "    print(' '.join(test_data[index].src))\n",
    "    print(''.join(test_data[index].trg))\n",
    "    src = ' '.join(test_data[index].src)\n",
    "    attns = beam_search(src, 1)\n",
    "    sns.heatmap(attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:zyc]",
   "language": "python",
   "name": "conda-env-zyc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
