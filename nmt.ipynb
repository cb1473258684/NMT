{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python自带\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 科学计算\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP 相关\n",
    "import jieba\n",
    "import torchtext\n",
    "from nltk import word_tokenize\n",
    "from nltk.translate import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tokenize_zh(input):\n",
    "    return list(jieba.cut(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.769 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "SRC = torchtext.data.Field(tokenize=word_tokenize)\n",
    "TRG = torchtext.data.Field(tokenize=word_tokenize_zh)\n",
    "\n",
    "train_data = torchtext.datasets.TranslationDataset(\n",
    "    path='data/news-commentary-v12.zh-en',\n",
    "    exts=('.en', '.zh'),\n",
    "    fields=(SRC, TRG)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227383\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = torchtext.datasets.TranslationDataset(\n",
    "    path='data/newsdev2017-enzh',\n",
    "    exts=('.en', '.zh'),\n",
    "    fields=(SRC, TRG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2002\n"
     ]
    }
   ],
   "source": [
    "print(len(dev_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torchtext.datasets.TranslationDataset(\n",
    "    path='data/newstest2017-enzh',\n",
    "    exts=('.en', '.zh'),\n",
    "    fields=(SRC, TRG)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n"
     ]
    }
   ],
   "source": [
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data.src, dev_data.src, test_data.src, min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG.build_vocab(train_data.trg, dev_data.trg, test_data.trg, min_freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95167\n",
      "91404\n"
     ]
    }
   ],
   "source": [
    "print(len(SRC.vocab.__dict__['freqs']))\n",
    "print(len(TRG.vocab.__dict__['freqs']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 2195167/2196018 [04:23<00:00, 8545.75it/s]"
     ]
    }
   ],
   "source": [
    "SRC.vocab.load_vectors(torchtext.vocab.Vectors('/home/zyc/Downloads/glove.840B.300d.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRG.vocab.load_vectors(torchtext.vocab.Vectors('/home/zyc/Downloads/sgns.target.word-word.dynwin5.thr10.neg5.dim300.iter5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    '''Encoder(bi-GRU)\n",
    "    '''\n",
    "    def __init__(self, pretrained_embed, padding_idx, fix, hidden_size,\n",
    "                 n_layers=1, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embed)\n",
    "        self.embedding.padding_idx = padding_idx\n",
    "        if fix:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.gru = nn.GRU(self.embedding.embedding_dim, hidden_size, n_layers,\n",
    "                            dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, src, hidden=None):\n",
    "        '''\n",
    "        Inputs:\n",
    "            src: input word index\n",
    "            hidden: h_t-1 (num_layers * num_directions, batch, hidden_size)\n",
    "        Outputs:\n",
    "            output: [T*B*H]\n",
    "            hidden: h_t\n",
    "        '''\n",
    "        embeded = self.embedding(src)\n",
    "        outputs, hidden = self.gru(embeded, hidden)\n",
    "        \n",
    "        # Sum bi-lstm outputs\n",
    "        output = (outputs[:, :, :self.hidden_size] + \n",
    "                   outputs[:, :, self.hidden_size:])\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "class ConcatAttn(nn.Module):\n",
    "    '''Attention(concat)\n",
    "    Params:\n",
    "        hidden_size: hidden size\n",
    "    '''\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ConcatAttn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1.0 / sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, hidden, encoder_output):\n",
    "        '''\n",
    "        Inputs:\n",
    "            hidden: [1*B*H] \n",
    "            encoder_output: [T*B*H]\n",
    "        Outputs:\n",
    "            energy: normalised weights [B*T]\n",
    "        '''\n",
    "        # Expand hidden [1*B*H] -> [T*B*H] -> [B*T*H]\n",
    "        hidden = hidden.repeat(encoder_output.size(0), 1, 1).transpose(0, 1)\n",
    "\n",
    "        # Transfer encoder_output to [B*T*H]\n",
    "        encoder_output = encoder_output.transpose(0, 1)\n",
    "\n",
    "        # Calculate energy and normalise  [B*T]\n",
    "        attn_energy = self.score(hidden, encoder_output)\n",
    "        return F.softmax(attn_energy)\n",
    "\n",
    "    def score(self, hidden, encoder_output):\n",
    "        '''\n",
    "        Inputs:\n",
    "            hidden: [B*T*H]\n",
    "            encoder_output: [B*T*H]\n",
    "        Outputs:\n",
    "            attn_energy: weights [B*T]\n",
    "        '''\n",
    "        # Project vectors [B*T*2H] -> [B*T*H] -> [B*H*T]\n",
    "        energy = self.attn(torch.cat([hidden, encoder_output], 2))\n",
    "        energy = energy.transpose(1, 2)\n",
    "        \n",
    "        # Expend v  [H] -> [B*H] -> [B*1*H]\n",
    "        v = self.v.repeat(encoder_output.size(0), 1).unsqueeze(1)\n",
    "        \n",
    "        # [B*1*H] * [B*H*T] -> [B*1*T]\n",
    "        attn_energy = torch.bmm(v, energy)\n",
    "        return attn_energy\n",
    "\n",
    "        \n",
    "class BilinearAttn(nn.Module):\n",
    "    '''Attention(bilinear)\n",
    "    Params:\n",
    "        hidden_size: hidden size\n",
    "    '''\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BilinearAttn, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bilinear = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    \n",
    "    def forward(self, hidden, encoder_output):\n",
    "        '''\n",
    "        Inputs:\n",
    "            hidden: [1*B*H] \n",
    "            encoder_output: [T*B*H]\n",
    "        Outputs:\n",
    "            energy: normalised weights [B*T]\n",
    "        '''\n",
    "        # [T*B*H] -> [T*B*H] -> [B*H*T]\n",
    "        wh = self.bilinear(encoder_output).permute(1, 2, 0)\n",
    "        \n",
    "        # [1*B*H] -> [B*1*H] x [B*H*T] => [B*1*T] -> [B*T]\n",
    "        score = hidden.transpose(0, 1).bmm(wh).squeeze(1)\n",
    "        \n",
    "        return F.softmax(score)\n",
    "    \n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''Decoder(bi-GRU)\n",
    "    '''\n",
    "    def __init__(self, pretrained_embed, padding_idx, hidden_size, \n",
    "                 n_layers=1, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embed)\n",
    "        self.embedding.padding_idx = padding_idx\n",
    "        if fix:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "            \n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        \n",
    "        self.attention = BilinearAttn(hidden_size)\n",
    "        \n",
    "        self.grucell = nn.GRUCell(\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_output):\n",
    "        '''\n",
    "        Inputs:\n",
    "            input: [B]\n",
    "            hidden: [1*B*H]\n",
    "            encoder_output: [T*B*H]\n",
    "        Outputs:\n",
    "            output: probabilities of prediction\n",
    "            hidden: [1*B*H]\n",
    "            attn_weights: [B*1*T]\n",
    "        '''\n",
    "        # [1*B*H]\n",
    "        embeded = self.embed(input).unsqueeze(0) \n",
    "        embeded = self.dropout(embeded)\n",
    "\n",
    "        # Calculate attention weights and apply  [B*1*T] * [T*B*H] -> [B*1*H] -> [1*B*H]\n",
    "        attn_weights = self.attention(hidden, encoder_output)\n",
    "        context = attn_weights.bmm(encoder_output.transpose(0, 1)).transpose(0, 1)\n",
    "\n",
    "        # Combine embeded input and attended context\n",
    "        rnn_input = torch.cat([embeded, context], 2)\n",
    "        output, hidden = self.gru(rnn_input, hidden)\n",
    "\n",
    "        # [1*B*H] -> [B*H]\n",
    "        output = output.squeeze(0)\n",
    "        context = context.squeeze(0)\n",
    "\n",
    "        # Output probabilities\n",
    "        output = self.out(torch.cat([output, context], 1))\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zyc",
   "language": "python",
   "name": "zyc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
